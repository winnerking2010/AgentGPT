{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdWFeVDmj63nPSmU1XqGur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winnerking2010/AgentGPT/blob/main/colab0325%E5%BE%AE%E8%B0%83DEEPSEEK%E8%92%B8%E9%A6%8Fllama8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸€ã€åŠ è½½unsloth"
      ],
      "metadata": {
        "id": "K3VLTznSjTRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYCzSrbkaSaH",
        "outputId": "bead2c38-f4f9-4905-8ddb-0329d145af98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.18)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.3.14 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.3.16)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.3)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.17)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.49.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.4.1)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.29.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**äºŒã€ä¸‹è½½unsloth 4bit é‡åŒ–å¤§æ¨¡å‹å’Œåˆ†è¯å™¨**"
      ],
      "metadata": {
        "id": "P4nlFoejjjJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#å‡å°‘åºåˆ—é•¿åº¦ä»2048åˆ°1024\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = 1024,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yisAT5BnbMny",
        "outputId": "7c3e21ac-ee26-40f6-d906-f347b8338d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ä¸‰ã€å°† LoRA çŸ©é˜µæ·»åŠ åˆ°é¢„è®­ç»ƒ LLMä¸­ï¼Œè¿™å°†æœ‰åŠ©äºå¾®è°ƒæ¨¡å‹çš„å“åº”**"
      ],
      "metadata": {
        "id": "GxVoYfhQkBtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,  # å¯è®¾ç½®ä¸ºä»»æ„å€¼ï¼Œä½†å»ºè®®è®¾ä¸º0ä»¥ä¼˜åŒ–æ€§èƒ½[2](@ref)\n",
        "    bias=\"none\",        # å»ºè®®è®¾ä¸º\"none\"ä»¥ä¼˜åŒ–æ€§èƒ½[5](@ref)\n",
        "    use_gradient_checkpointing=\"unsloth\",  # æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ[2](@ref)\n",
        "    random_state=3977,\n",
        "    use_rslora=False,  # Unslothä¹Ÿæ”¯æŒç§©ç¨³å®šLoRA[5](@ref)\n",
        "    loftq_config=None   # å¯é€‰LoftQé…ç½®\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyVW3YNgePcQ",
        "outputId": "07f9a7fd-24da-4d02-ae32-4dd55304c678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.3.18 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**å››ã€Promptçš„ç»“æ„**"
      ],
      "metadata": {
        "id": "SpzF6qFroBhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "X9VtzGgXoG4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**äº”ã€åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†æ­£ç¡®åœ°æ„å»º alpaca_promptä¸­çš„æ‰€æœ‰æ•°æ®**"
      ],
      "metadata": {
        "id": "LfNKxbFpoiWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # æ ‡è®°åºåˆ—ç»“æŸï¼Œé˜²æ­¢ç”Ÿæˆæ— é™å»¶ä¼¸[1](@ref)\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"å°†å¯¹è¯æ•°æ®è½¬æ¢ä¸ºå¸¦EOSæ ‡è®°çš„è¾“å…¥æ ¼å¼\"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    # æŒ‰æ¨¡æ¿ç”Ÿæˆå¸¦EOSçš„è¾“å…¥æ–‡æœ¬\n",
        "    for instruction, input_text, output_text in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(\n",
        "            instruction=instruction,\n",
        "            input=input_text,\n",
        "            output=output_text\n",
        "        ) + EOS_TOKEN  # å¿…é¡»æ·»åŠ EOS_TOKEN[1](@ref)\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}"
      ],
      "metadata": {
        "id": "PBwgkkqCopcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**å…­ã€åŠ è½½å°†ç”¨äºå¾®è°ƒæ¨¡å‹çš„æ•°æ®é›†**"
      ],
      "metadata": {
        "id": "K28zjUHVozpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æ·»åŠ ç±»å‹æç¤ºï¼ˆå¯é€‰ï¼‰\n",
        "from datasets import DatasetDict\n",
        "\n",
        "def load_and_preprocess():\n",
        "    \"\"\"åŠ è½½å¹¶é¢„å¤„ç†Alpacaæ•°æ®é›†\"\"\"\n",
        "    dataset = load_dataset(\n",
        "        \"yahma/alpaca-cleaned\",\n",
        "        split=\"train\"\n",
        "    )\n",
        "    return dataset.map(\n",
        "        formatting_prompts_func,\n",
        "        batched=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "UlqzjBRho5pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ä¸ƒã€ä½¿ç”¨ SFTTrainerå’Œ hyperparameter åˆå§‹åŒ–trainer**"
      ],
      "metadata": {
        "id": "xSY0_CYBvTnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j3nWk4fwHQb",
        "outputId": "5e47f98f-97dd-4da0-d44f-60a50c67fd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.4.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.49.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.11.14)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH8k9mHd52l0",
        "outputId": "8588c4d5-0ab5-4cd5-83c6-6935142f8250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ä»£ç æŒç»­æŠ¥é”™ GPUæº¢å‡º è¶…å‡ºå†…å­˜ï¼Ÿï¼Ÿ\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer  # [1,3,4](@ref)\n",
        "from transformers import TrainingArguments  # [1,5,6](@ref)\n",
        "from unsloth import is_bfloat16_supported  # [4,7](@ref)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # [1,3,5](@ref)\n",
        "    tokenizer=tokenizer,  # [1,3,5](@ref)\n",
        "    train_dataset=datasets,  # [1,3,5](@ref)\n",
        "    datasets_text_field=\"text\",  # [1,3,5](@ref)\n",
        "    max_seq_length=max_seq_length,  # [1,3,5](@ref)\n",
        "    datasets_num_proc=2,  # [1,3,5](@ref)\n",
        "    packing=False,  # [3,5](@ref)\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # [1,5,6](@ref)\n",
        "        gradient_accumulation_steps=4,  # [1,5,6](@ref)\n",
        "        warmup_steps=5,  # [1,5,6](@ref)\n",
        "        max_steps=120,  # [1,5](@ref)\n",
        "        learning_rate=2e-4,  # [1,5,6](@ref)\n",
        "        fp16=not is_bfloat16_supported(),  # [1,4,7](@ref)\n",
        "        bf16=is_bfloat16_supported(),  # [1,4,7](@ref)\n",
        "        logging_steps=1,  # [1,5,6](@ref)\n",
        "        optim=\"adamw_8bit\",  # [1,5,6](@ref)\n",
        "        weight_decay=0.01,  # [1,5,6](@ref)\n",
        "        lr_scheduler_type=\"linear\",  # [1,5,6](@ref)\n",
        "        seed=3407,  # [1,3,5](@ref)\n",
        "        output_dir=\"outputs\",  # [1,3,5](@ref)\n",
        "        report_to=\"none\",  # [1,3,5](@ref)\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "NMxnxlS7vdE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æŠ¥é”™ï¼ï¼\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1d4Rrvf4_5cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TypeError: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œè®¾ç½® llm_int8_enable_fp32_cpu_offload å’Œ device_map\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "g32Aj9nTBAmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 39053 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 6.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "GZHRf0J-BxR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 39053 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 6.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œå‡å°æ‰¹é‡å¤§å°ï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # å‡å°æ‰¹é‡å¤§å°\n",
        "    gradient_accumulation_steps=8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")"
      ],
      "metadata": {
        "id": "z7flOYb-CS6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ä¼˜åŒ–æ§åˆ¶GPUåœ¨14Gä»¥å†…\n",
        "# ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°ï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # è¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°\n",
        "    gradient_accumulation_steps=16,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n"
      ],
      "metadata": {
        "id": "JqfOKb-eDWcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**å…«ã€åŠ è½½ peft**"
      ],
      "metadata": {
        "id": "Vz2fdgCXKKnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxJPixWfFvWp",
        "outputId": "7a24bde1-75d6-40e0-f202-491d1e7b1130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.49.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨\n",
        "config = LoraConfig(\n",
        "    r=8,  # LoRA ç§©\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # éœ€è¦è°ƒæ•´çš„ç›®æ ‡æ¨¡å—\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°ï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # è¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°\n",
        "    gradient_accumulation_steps=16,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 3588 has 14.71 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 7.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "\n"
      ],
      "metadata": {
        "id": "ETvyBwodF9_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=1024,  # å‡å°‘åºåˆ—é•¿åº¦\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨\n",
        "config = LoraConfig(\n",
        "    r=4,  # é™ä½ LoRA ç§©\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°ï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # è¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°\n",
        "    gradient_accumulation_steps=32,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=1024,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 3588 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 11.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      ],
      "metadata": {
        "id": "zKNvc0xYGcVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ç¬¬å…­æ¬¡ä¼˜åŒ– æ§åˆ¶å†…å­˜\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=512,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ r å€¼\n",
        "config = LoraConfig(\n",
        "    r=2,  # é™ä½ LoRA ç§©\n",
        "    lora_alpha=4,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°å’Œå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # ä¿æŒå°æ‰¹é‡\n",
        "    gradient_accumulation_steps=64,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    for step in range(training_args.max_steps):\n",
        "        trainer.train_step()\n",
        "        if step % 10 == 0:  # æ¯ 10 æ­¥æ¸…é™¤ä¸€æ¬¡ç¼“å­˜\n",
        "            torch.cuda.empty_cache()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
        "\n",
        "#AttributeError: 'UnslothSFTTrainer' object has no attribute 'train_step'\n"
      ],
      "metadata": {
        "id": "xYQBXWPRG_ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ä¹ã€åŠ è½½è®­ç»ƒæ•°æ® è¿­ä»£ä¼˜åŒ–**"
      ],
      "metadata": {
        "id": "T6VF78bsK48P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=512,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ r å€¼\n",
        "config = LoraConfig(\n",
        "    r=2,  # é™ä½ LoRA ç§©\n",
        "    lora_alpha=4,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å‡å°æ‰¹é‡å¤§å°å’Œå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # ä¿æŒå°æ‰¹é‡\n",
        "    gradient_accumulation_steps=64,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "    # æ¯è®­ç»ƒä¸€å®šæ­¥æ•°åæ¸…é™¤ç¼“å­˜\n",
        "    torch.cuda.empty_cache()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 89773 has 14.71 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
      ],
      "metadata": {
        "id": "U7C4pDBRK-vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ç¬¬10æ¬¡ä¼˜åŒ–  æ§åˆ¶å†…å­˜\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå†æ¬¡é™ä½ max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ r å’Œ lora_alpha å€¼\n",
        "config = LoraConfig(\n",
        "    r=1,  # å†æ¬¡é™ä½ LoRA ç§©\n",
        "    lora_alpha=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # ä¿æŒå°æ‰¹é‡\n",
        "    gradient_accumulation_steps=128,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=256,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "    # æ¯è®­ç»ƒä¸€å®šæ­¥æ•°åæ¸…é™¤ç¼“å­˜\n",
        "    torch.cuda.empty_cache()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
        "\n",
        "#å‘ç”Ÿå…¶ä»–é”™è¯¯: element 0 of tensors does not require grad and does not have a grad_fn\n"
      ],
      "metadata": {
        "id": "2RPdxMa4LglQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ç¬¬11æ¬¡ä¼˜åŒ–  æ¢¯åº¦åå‘æ›´æ–°\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå†æ¬¡é™ä½ max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ r å’Œ lora_alpha å€¼\n",
        "config = LoraConfig(\n",
        "    r=1,  # å†æ¬¡é™ä½ LoRA ç§©\n",
        "    lora_alpha=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# æ£€æŸ¥å¹¶ç¡®ä¿åªæœ‰ LoRA é€‚é…å™¨çš„å‚æ•°éœ€è¦æ¢¯åº¦\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼Œç”¨äºè°ƒè¯•\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # ä¿æŒå°æ‰¹é‡\n",
        "    gradient_accumulation_steps=128,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=256,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "    # æ¯è®­ç»ƒä¸€å®šæ­¥æ•°åæ¸…é™¤ç¼“å­˜\n",
        "    torch.cuda.empty_cache()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n"
      ],
      "metadata": {
        "id": "FsL1niE1Oy97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ç¬¬12æ¬¡\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# å®šä¹‰è‡ªå®šä¹‰çš„ device_map\n",
        "device_map = {\n",
        "    \"\": 0  # å‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ•´ä¸ªæ¨¡å‹åˆ†é…åˆ°è¯¥GPU\n",
        "}\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå†æ¬¡é™ä½ max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    #PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ï¼Œè¿›ä¸€æ­¥é™ä½ r å’Œ lora_alpha å€¼\n",
        "config = LoraConfig(\n",
        "    r=1,  # å†æ¬¡é™ä½ LoRA ç§©\n",
        "    lora_alpha=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# ç¡®ä¿åªæœ‰ LoRA é€‚é…å™¨çš„å‚æ•°éœ€è¦æ¢¯åº¦\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# è¿‡æ»¤å‡ºå¯è®­ç»ƒçš„å‚æ•°\n",
        "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
        "\n",
        "# å®šä¹‰è®­ç»ƒå‚æ•°ï¼Œè¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # ä¿æŒå°æ‰¹é‡\n",
        "    gradient_accumulation_steps=128,  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # ä½¿ç”¨ 8bit çš„ AdamW ä¼˜åŒ–å™¨\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=5000  # å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…ä¿å­˜ä¸­é—´æ¨¡å‹å ç”¨è¿‡å¤šå†…å­˜\n",
        ")\n",
        "\n",
        "# åˆ›å»º SFTTrainerï¼Œæ˜ç¡®ä¼ é€’å¯è®­ç»ƒå‚æ•°\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    max_seq_length=256,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        "    trainable_params=trainable_params  # ä¼ é€’å¯è®­ç»ƒå‚æ•°\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†ï¼Œè¿™é‡Œä»¥åŠ è½½ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ä¸ºä¾‹\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# å¯¹æ•°æ®é›†è¿›è¡Œç®€å•å¤„ç†ï¼Œå‡è®¾æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µåä¸º \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "# ä»¥æµå¼æ–¹å¼å¤„ç†æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "try:\n",
        "    trainer.train()\n",
        "    # æ¯è®­ç»ƒä¸€å®šæ­¥æ•°åæ¸…é™¤ç¼“å­˜\n",
        "    torch.cuda.empty_cache()\n",
        "    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
        "    model.save_pretrained(\"deepseekoutputsllama8B\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ã€‚\")\n",
        "        torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜\n",
        "    else:\n",
        "        print(f\"å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 123549 has 14.72 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 18.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
      ],
      "metadata": {
        "id": "LLc41HybPi3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
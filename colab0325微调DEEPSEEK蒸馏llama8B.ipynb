{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdWFeVDmj63nPSmU1XqGur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winnerking2010/AgentGPT/blob/main/colab0325%E5%BE%AE%E8%B0%83DEEPSEEK%E8%92%B8%E9%A6%8Fllama8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "一、加载unsloth"
      ],
      "metadata": {
        "id": "K3VLTznSjTRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYCzSrbkaSaH",
        "outputId": "bead2c38-f4f9-4905-8ddb-0329d145af98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.18)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.3.14 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.3.16)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.3)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.17)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.49.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.4.1)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.29.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**二、下载unsloth 4bit 量化大模型和分词器**"
      ],
      "metadata": {
        "id": "P4nlFoejjjJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#减少序列长度从2048到1024\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = 1024,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yisAT5BnbMny",
        "outputId": "7c3e21ac-ee26-40f6-d906-f347b8338d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**三、将 LoRA 矩阵添加到预训练 LLM中，这将有助于微调模型的响应**"
      ],
      "metadata": {
        "id": "GxVoYfhQkBtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,  # 可设置为任意值，但建议设为0以优化性能[2](@ref)\n",
        "    bias=\"none\",        # 建议设为\"none\"以优化性能[5](@ref)\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 支持超长上下文训练[2](@ref)\n",
        "    random_state=3977,\n",
        "    use_rslora=False,  # Unsloth也支持秩稳定LoRA[5](@ref)\n",
        "    loftq_config=None   # 可选LoftQ配置\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyVW3YNgePcQ",
        "outputId": "07f9a7fd-24da-4d02-ae32-4dd55304c678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.3.18 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**四、Prompt的结构**"
      ],
      "metadata": {
        "id": "SpzF6qFroBhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "X9VtzGgXoG4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**五、创建了一个函数，它将正确地构建 alpaca_prompt中的所有数据**"
      ],
      "metadata": {
        "id": "LfNKxbFpoiWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # 标记序列结束，防止生成无限延伸[1](@ref)\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"将对话数据转换为带EOS标记的输入格式\"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    # 按模板生成带EOS的输入文本\n",
        "    for instruction, input_text, output_text in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(\n",
        "            instruction=instruction,\n",
        "            input=input_text,\n",
        "            output=output_text\n",
        "        ) + EOS_TOKEN  # 必须添加EOS_TOKEN[1](@ref)\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}"
      ],
      "metadata": {
        "id": "PBwgkkqCopcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**六、加载将用于微调模型的数据集**"
      ],
      "metadata": {
        "id": "K28zjUHVozpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 添加类型提示（可选）\n",
        "from datasets import DatasetDict\n",
        "\n",
        "def load_and_preprocess():\n",
        "    \"\"\"加载并预处理Alpaca数据集\"\"\"\n",
        "    dataset = load_dataset(\n",
        "        \"yahma/alpaca-cleaned\",\n",
        "        split=\"train\"\n",
        "    )\n",
        "    return dataset.map(\n",
        "        formatting_prompts_func,\n",
        "        batched=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "UlqzjBRho5pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**七、使用 SFTTrainer和 hyperparameter 初始化trainer**"
      ],
      "metadata": {
        "id": "xSY0_CYBvTnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j3nWk4fwHQb",
        "outputId": "5e47f98f-97dd-4da0-d44f-60a50c67fd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.4.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.49.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.11.14)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH8k9mHd52l0",
        "outputId": "8588c4d5-0ab5-4cd5-83c6-6935142f8250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#代码持续报错 GPU溢出 超出内存？？\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer  # [1,3,4](@ref)\n",
        "from transformers import TrainingArguments  # [1,5,6](@ref)\n",
        "from unsloth import is_bfloat16_supported  # [4,7](@ref)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # [1,3,5](@ref)\n",
        "    tokenizer=tokenizer,  # [1,3,5](@ref)\n",
        "    train_dataset=datasets,  # [1,3,5](@ref)\n",
        "    datasets_text_field=\"text\",  # [1,3,5](@ref)\n",
        "    max_seq_length=max_seq_length,  # [1,3,5](@ref)\n",
        "    datasets_num_proc=2,  # [1,3,5](@ref)\n",
        "    packing=False,  # [3,5](@ref)\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # [1,5,6](@ref)\n",
        "        gradient_accumulation_steps=4,  # [1,5,6](@ref)\n",
        "        warmup_steps=5,  # [1,5,6](@ref)\n",
        "        max_steps=120,  # [1,5](@ref)\n",
        "        learning_rate=2e-4,  # [1,5,6](@ref)\n",
        "        fp16=not is_bfloat16_supported(),  # [1,4,7](@ref)\n",
        "        bf16=is_bfloat16_supported(),  # [1,4,7](@ref)\n",
        "        logging_steps=1,  # [1,5,6](@ref)\n",
        "        optim=\"adamw_8bit\",  # [1,5,6](@ref)\n",
        "        weight_decay=0.01,  # [1,5,6](@ref)\n",
        "        lr_scheduler_type=\"linear\",  # [1,5,6](@ref)\n",
        "        seed=3407,  # [1,3,5](@ref)\n",
        "        output_dir=\"outputs\",  # [1,3,5](@ref)\n",
        "        report_to=\"none\",  # [1,3,5](@ref)\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "NMxnxlS7vdE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 报错！！\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 定义训练参数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1d4Rrvf4_5cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TypeError: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器，设置 llm_int8_enable_fp32_cpu_offload 和 device_map\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 定义训练参数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "g32Aj9nTBAmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 39053 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 6.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 定义训练参数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "GZHRf0J-BxR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 39053 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 6.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 定义训练参数，减小批量大小，增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 减小批量大小\n",
        "    gradient_accumulation_steps=8,  # 增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")"
      ],
      "metadata": {
        "id": "z7flOYb-CS6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#优化控制GPU在14G以内\n",
        "# ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步减小批量大小，增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 进一步减小批量大小\n",
        "    gradient_accumulation_steps=16,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n"
      ],
      "metadata": {
        "id": "JqfOKb-eDWcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**八、加载 peft**"
      ],
      "metadata": {
        "id": "Vz2fdgCXKKnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxJPixWfFvWp",
        "outputId": "7a24bde1-75d6-40e0-f202-491d1e7b1130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.49.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器\n",
        "config = LoraConfig(\n",
        "    r=8,  # LoRA 秩\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # 需要调整的目标模块\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=2048)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步减小批量大小，增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 进一步减小批量大小\n",
        "    gradient_accumulation_steps=16,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 3588 has 14.71 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 7.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
        "\n"
      ],
      "metadata": {
        "id": "ETvyBwodF9_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=1024,  # 减少序列长度\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器\n",
        "config = LoraConfig(\n",
        "    r=4,  # 降低 LoRA 秩\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步减小批量大小，增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 进一步减小批量大小\n",
        "    gradient_accumulation_steps=32,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=1024,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 3588 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 11.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      ],
      "metadata": {
        "id": "zKNvc0xYGcVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#第六次优化 控制内存\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器，进一步降低 max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=512,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器，进一步降低 r 值\n",
        "config = LoraConfig(\n",
        "    r=2,  # 降低 LoRA 秩\n",
        "    lora_alpha=4,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步减小批量大小和增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 保持小批量\n",
        "    gradient_accumulation_steps=64,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    for step in range(training_args.max_steps):\n",
        "        trainer.train_step()\n",
        "        if step % 10 == 0:  # 每 10 步清除一次缓存\n",
        "            torch.cuda.empty_cache()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n",
        "\n",
        "#AttributeError: 'UnslothSFTTrainer' object has no attribute 'train_step'\n"
      ],
      "metadata": {
        "id": "xYQBXWPRG_ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**九、加载训练数据 迭代优化**"
      ],
      "metadata": {
        "id": "T6VF78bsK48P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器，进一步降低 max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=512,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器，进一步降低 r 值\n",
        "config = LoraConfig(\n",
        "    r=2,  # 降低 LoRA 秩\n",
        "    lora_alpha=4,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步减小批量大小和增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 保持小批量\n",
        "    gradient_accumulation_steps=64,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "    # 每训练一定步数后清除缓存\n",
        "    torch.cuda.empty_cache()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 89773 has 14.71 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
      ],
      "metadata": {
        "id": "U7C4pDBRK-vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#第10次优化  控制内存\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器，再次降低 max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器，进一步降低 r 和 lora_alpha 值\n",
        "config = LoraConfig(\n",
        "    r=1,  # 再次降低 LoRA 秩\n",
        "    lora_alpha=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 保持小批量\n",
        "    gradient_accumulation_steps=128,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=256,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "    # 每训练一定步数后清除缓存\n",
        "    torch.cuda.empty_cache()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n",
        "\n",
        "#发生其他错误: element 0 of tensors does not require grad and does not have a grad_fn\n"
      ],
      "metadata": {
        "id": "2RPdxMa4LglQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#第11次优化  梯度反向更新\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器，再次降低 max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器，进一步降低 r 和 lora_alpha 值\n",
        "config = LoraConfig(\n",
        "    r=1,  # 再次降低 LoRA 秩\n",
        "    lora_alpha=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 检查并确保只有 LoRA 适配器的参数需要梯度\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# 打印可训练参数，用于调试\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 定义训练参数，进一步增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 保持小批量\n",
        "    gradient_accumulation_steps=128,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=10000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    max_seq_length=256,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "    # 每训练一定步数后清除缓存\n",
        "    torch.cuda.empty_cache()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"outputs\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n"
      ],
      "metadata": {
        "id": "FsL1niE1Oy97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#第12次\n",
        "import torch\n",
        "import datasets\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义自定义的 device_map\n",
        "device_map = {\n",
        "    \"\": 0  # 假设只有一个GPU，将整个模型分配到该GPU\n",
        "}\n",
        "\n",
        "# 加载模型和分词器，再次降低 max_seq_length\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    #PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# 为模型添加 LoRA 适配器，进一步降低 r 和 lora_alpha 值\n",
        "config = LoraConfig(\n",
        "    r=1,  # 再次降低 LoRA 秩\n",
        "    lora_alpha=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# 确保只有 LoRA 适配器的参数需要梯度\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# 过滤出可训练的参数\n",
        "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
        "\n",
        "# 定义训练参数，进一步增加梯度累积步数\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # 保持小批量\n",
        "    gradient_accumulation_steps=128,  # 进一步增加梯度累积步数\n",
        "    warmup_steps=5,\n",
        "    max_steps=120,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",  # 使用 8bit 的 AdamW 优化器\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",\n",
        "    save_steps=5000  # 减少保存频率，避免保存中间模型占用过多内存\n",
        ")\n",
        "\n",
        "# 创建 SFTTrainer，明确传递可训练参数\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    max_seq_length=256,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        "    trainable_params=trainable_params  # 传递可训练参数\n",
        ")\n",
        "\n",
        "# 加载数据集，这里以加载一个示例数据集为例\n",
        "train_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# 对数据集进行简单处理，假设数据集中的文本字段名为 \"text\"\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "# 以流式方式处理数据集，减少内存占用\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=train_dataset.column_names)\n",
        "\n",
        "# 开始训练\n",
        "try:\n",
        "    trainer.train()\n",
        "    # 每训练一定步数后清除缓存\n",
        "    torch.cuda.empty_cache()\n",
        "    # 保存微调后的模型\n",
        "    model.save_pretrained(\"deepseekoutputsllama8B\")\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"GPU 内存不足，请尝试进一步调整参数。\")\n",
        "        torch.cuda.empty_cache()  # 清除缓存\n",
        "    else:\n",
        "        print(f\"发生其他错误: {e}\")\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 123549 has 14.72 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 18.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
      ],
      "metadata": {
        "id": "LLc41HybPi3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}